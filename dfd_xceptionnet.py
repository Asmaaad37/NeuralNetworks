# -*- coding: utf-8 -*-
"""DFD - XceptionNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AxtEW9YNBWid85dvC9HUE7G1VGrTsCyA

**Project Title: Deep fake Detection Using Neural Networks**

**Objective:**


Build a robust detection system to identify deep fake videos using AI-driven artifact analysis and machine learning techniques.

***Project Overview***


This project aims to develop an end-to-end pipeline for deep fake video detection.
The workflow is divided into four phases: **Data Preparation, Feature Extraction, Model Development, and Deployment.** Each phase will be handled by a dedicated team member, ensuring clear responsibilities and streamlined collaboration.

**Model Architecture:** Xception-based CNN for binary classification (Real vs Fake videos)

**Dataset:** FaceForensics++ C23 dataset with deepfake and original video samples

**Approach:** Frame-based detection with random sampling strategy

**Importing Libraries**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torchinfo

pip install timm

import torch
torch.cuda.is_available()

!nvidia-smi

# Import libraries
import os
import numpy as np
import tensorflow as tf
import cv2
import dlib
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras.applications import Xception
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from google.colab import drive
from tqdm import tqdm

import sklearn.model_selection
import torch
import torch.nn as nn
import torch.optim as optim
import torchinfo
import torchvision
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from torch.utils.data import DataLoader
from torch.utils.data import DataLoader, random_split
from torchinfo import summary
from tensorflow.keras.models import load_model
import cv2
from torchvision import datasets, transforms
from tqdm import tqdm

import os
import random
import cv2
import numpy as np

# Define the paths to video directories
deepfake_video_dir = "/content/drive/My Drive/deepfake_detection/train/Deepfake"
original_video_dir = "/content/drive/My Drive/deepfake_detection/train/original"

# Define the paths to store extracted frames
deepfake_frames_dir = "/content/drive/My Drive/deepfake_detection/train/deepfake_frames"
original_frames_dir = "/content/drive/My Drive/deepfake_detection/train/original_frames"

# Number of frames to extract from each video
num_frames_to_extract = 100

def extract_random_frames(video_path, output_folder, num_frames=100):
    """
    Extract random frames from a video and save them to the output folder.

    Args:
        video_path: Path to the video file
        output_folder: Path to save the extracted frames
        num_frames: Number of frames to extract
    """

    os.makedirs(output_folder, exist_ok=True)

    # Open the video
    cap = cv2.VideoCapture(video_path)

    # Check if video opened successfully
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return

    # Get video properties
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total_frames <= 0:
        print(f"Error: Could not determine frame count for {video_path}")
        return

    print(f"Video {os.path.basename(video_path)} has {total_frames} frames")

    # Generate random frame indices to extract
    # If we have fewer frames than requested, take all frames
    if total_frames <= num_frames:
        frames_to_extract = list(range(total_frames))
        print(f"Video has fewer frames than requested. Extracting all {total_frames} frames.")
    else:
        frames_to_extract = sorted(random.sample(range(total_frames), num_frames))

    # Extract the frames
    frame_count = 0
    extracted_count = 0

    for frame_idx in frames_to_extract:
        # Set the frame position
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)

        # Read the frame
        ret, frame = cap.read()

        if not ret:
            print(f"Failed to read frame {frame_idx}")
            continue

        # Save the frame
        frame_filename = f"frame_{frame_idx:06d}.jpg"
        frame_path = os.path.join(output_folder, frame_filename)
        cv2.imwrite(frame_path, frame)

        extracted_count += 1

    print(f"Extracted {extracted_count} frames from {os.path.basename(video_path)}")

    # Release the video capture
    cap.release()

def process_videos(video_dir, frames_dir):
    """
    Process all video files in a directory and extract frames.

    Args:
        video_dir: Directory containing videos
        frames_dir: Directory to store extracted frames
    """
    print(f"Processing videos from: {video_dir}")

    # Create frames directory if it doesn't exist
    os.makedirs(frames_dir, exist_ok=True)

    # Get all video files
    video_files = [f for f in os.listdir(video_dir) if os.path.isfile(os.path.join(video_dir, f)) and
                  f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]

    print(f"Found {len(video_files)} video files")

    for video_file in video_files:
        video_path = os.path.join(video_dir, video_file)

        # Create a folder for frames from this video
        video_name = os.path.splitext(video_file)[0]
        output_folder = os.path.join(frames_dir, video_name)

        print(f"Processing video: {video_file}")
        print(f"Extracting frames to: {output_folder}")

        # Extract frames from the video
        extract_random_frames(video_path, output_folder, num_frames_to_extract)

def main():
    # Process deepfake videos
    print("Starting to process deepfake videos...")
    process_videos(deepfake_video_dir, deepfake_frames_dir)

    # Process original videos
    print("Starting to process original videos...")
    process_videos(original_video_dir, original_frames_dir)

    print("Processing complete!")

if __name__ == "__main__":
    # Confirm before running
    confirm = input("This script will extract 100 random frames from each video. Continue? (y/n): ")
    if confirm.lower() == 'y':
        main()
    else:
        print("Operation cancelled.")

import os
import randomy
import matplotlib.pyplot as plt
from PIL import Image

# Paths
deepfake_path = "/content/drive/My Drive/deepfake_detection/train/train_fr/deepfake_frames"
original_path = "/content/drive/My Drive/deepfake_detection/train/train_fr/original_frames"

def get_random_frames(category_path, num_frames=5):
    frames = []
    video_folders = [os.path.join(category_path, d) for d in os.listdir(category_path) if os.path.isdir(os.path.join(category_path, d))]
    random.shuffle(video_folders)

    for folder in video_folders:
        images = [img for img in os.listdir(folder) if img.endswith(('.jpg', '.png'))]
        if images:
            frame_path = os.path.join(folder, random.choice(images))
            frames.append(frame_path)
        if len(frames) >= num_frames:
            break
    return frames

# Get random frames
deepfake_frames = get_random_frames(deepfake_path)
original_frames = get_random_frames(original_path)

# Display frames
def display_frames(frames, title):
    plt.figure(figsize=(15, 3))
    for i, frame_path in enumerate(frames):
        img = Image.open(frame_path)
        plt.subplot(1, len(frames), i + 1)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"{title} {i+1}")
    plt.tight_layout()
    plt.show()

display_frames(deepfake_frames, "Deepfake")
display_frames(original_frames, "Original")

"""**Connecting the Main Directories**"""

main_dir = "/content/drive/My Drive/deepfake_detection/train"

train_dir = os.path.join(main_dir,"train_fr")

classes = os.listdir(train_dir)
print(classes)

"""**MODEL ARCHITECTURE & TRAINING**





*   Xception Model Data Preprocessing Pipeline




"""

#  # Transforming to match the requirements of XceptionNet
class ConvertToRGB(object):
    def __call__(self, img):
        if img.mode != "RGB":
            img = img.convert("RGB")
        return img

# trnaforming to mach the requirements of XceptionNet
transformed_normalized = transforms.Compose(
    [
        ConvertToRGB(),
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Normalize(mean = [0.485, 0.456, 0.406], std  = [0.229, 0.224, 0.225]
)
    ]
)

"""**Typical usage in DataLoader**"""

dataset = datasets.ImageFolder(root=train_dir,transform=transformed_normalized)

print("Length of dataset ",dataset)

dataset.classes

train_dataset, val_dataset = random_split(dataset,[0.8,0.2])

print(f"Length of training set: {len(train_dataset)}")
print(f"Length of validation set: {len(val_dataset)}")

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

val_loader = DataLoader(val_dataset,batch_size=32,shuffle=False)

print(type(train_loader))
print(type(val_loader))

print("Batch shape ", next(iter(train_loader))[0].shape)

length_dataset = len(dataset)
length_train = len(train_dataset)
length_val = len(val_dataset)

percent_train = np.round(100 * length_train / length_dataset, 2)
percent_val = np.round(100 * length_val / length_dataset, 2)

print(f"Train data is {percent_train}% of full data")
print(f"Validation data is {percent_val}% of full data")

data_iter = iter(train_loader)
images, labels = next(data_iter)

# This gives you [batch_size, channels, height, width] for images
image_shape = images.shape
print("Shape of batch of images", image_shape)

# This gives you [batch_size] for labels
label_shape = labels.shape
print("Shape of batch of labels:", label_shape)

labels

"""**Xception Model Architecture Definition**"""

import torch
import torch.nn as nn
import torch.optim as optim
import timm  # PyTorch Image Models

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pretrained Xception model
model = timm.create_model('xception', pretrained=True)

# Modify the classifier head for binary classification
model.fc = nn.Sequential(
    nn.Dropout(p=0.3),
    nn.Linear(model.fc.in_features, 1),
    nn.Sigmoid()
)

model = model.to(device)

print(model)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.optim import Adam
import timm
from google.colab import files
from tqdm import tqdm
from torch.cuda.amp import GradScaler, autocast

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

"""**Training Loop Implementation**"""

from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import shutil

# Set paths
EPOCHS = 2
save_path = 'Xception_ft.pth'  # Changed to .pth for PyTorch compatibility

# Training loop
for epoch in range(EPOCHS):
    model.train()
    train_loss = 0
    train_correct = 0
    train_total = 0

    print(f"\n🟢 Epoch {epoch+1}/{EPOCHS} -------------------------")
    train_bar = tqdm(train_loader, desc="Training", leave=False)

    for images, labels in train_bar:
        images = images.to(device)
        labels = labels.float().unsqueeze(1).to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        preds = (outputs > 0.5).int()
        train_correct += (preds == labels.int()).sum().item()
        train_total += labels.size(0)

    avg_train_loss = train_loss / len(train_loader)
    train_acc = train_correct / train_total

    # Validation loop
    model.eval()
    val_loss = 0
    val_correct = 0
    val_total = 0

    with torch.no_grad():
        val_bar = tqdm(val_loader, desc="Validating", leave=False)
        for images, labels in val_bar:
            images = images.to(device)
            labels = labels.float().unsqueeze(1).to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            preds = (outputs > 0.5).int()
            val_correct += (preds == labels.int()).sum().item()
            val_total += labels.size(0)

    avg_val_loss = val_loss / len(val_loader)
    val_acc = val_correct / val_total

    print(f"📉 Training Accuracy: {train_acc:.4f}, Training Loss: {avg_train_loss:.4f}")
    print(f"📊 Validation Accuracy: {val_acc:.4f}, Validation Loss: {avg_val_loss:.4f}")

# Save final model
torch.save(model.state_dict(), save_path)
print(f"\n💾 Final model saved to: {save_path}")

from google.colab import files
files.download('Xception_ft.pth')

from tensorflow import keras

from tensorflow.keras.applications import Xception
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
import tensorflow as tf

def build_xception_model():
    base_model = Xception(weights=None, include_top=False, input_shape=(299, 299, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(1, activation='sigmoid')(x)  # Binary classification
    model = Model(inputs=base_model.input, outputs=x)
    return model

model_xception = build_xception_model()
model_xception.load_weights("Xception_ft.weights.h5")

import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

# 📌 STEP 3: Load models

def preprocess_frame(frame, target_size):
    frame = cv2.resize(frame, target_size)
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = frame / 255.0  # normalize
    frame = np.expand_dims(frame, axis=0)
    return frame

# 📌 STEP 5: Load video and predict

def predict_video(video_path):
    cap = cv2.VideoCapture(video_path)
    xception_scores = []

    frame_skip = 5  # Use every 5th frame for speed

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % frame_skip != 0:
            frame_count += 1
            continue

        # Preprocess for both models
        frame_xception = preprocess_frame(frame, (299, 299))


        # Predict
        x_pred = model_xception.predict(frame_xception, verbose=0)[0][0]


        xception_scores.append(x_pred)
        frame_count += 1

    cap.release()

    # Aggregate results
    x_avg = np.mean(xception_scores)

    print(f"\n✅ Results on {video_path}")
    print(f"🔹 Xception Model Avg Deepfake Score: {x_avg:.4f}")

predict_video("/content/drive/MyDrive/deepfake_detection/test/videos/000_003.mp4")

# pip install onnx-tf

# onnx-tf convert -i xception.onnx -o xception_tf

files.download('Xception_ft.weights.h5')